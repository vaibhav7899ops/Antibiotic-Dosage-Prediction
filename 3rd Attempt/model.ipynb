{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7ec5cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[k-pick] best k=3 (silhouette on sample ≈ 0.864)\n",
      "\n",
      "=== Supervised performance on pseudo-labels (held-out) ===\n",
      "Accuracy:  0.934\n",
      "Precision: 0.949\n",
      "Recall:    0.859\n",
      "F1:        0.902\n",
      "\n",
      "Confusion matrix:\n",
      " [[2080   53]\n",
      " [ 163  991]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.927     0.975     0.951      2133\n",
      "           1      0.949     0.859     0.902      1154\n",
      "\n",
      "    accuracy                          0.934      3287\n",
      "   macro avg      0.938     0.917     0.926      3287\n",
      "weighted avg      0.935     0.934     0.933      3287\n",
      "\n",
      "\n",
      "Saved model bundle to: antibiotic_direction_pipeline.joblib\n",
      "Wrote sample predictions to: sample_predictions_first200.csv\n"
     ]
    }
   ],
   "source": [
    "# antibiotic_direction_pipeline.py\n",
    "# Unsupervised (KMeans) -> pseudo-labels (increase/decrease) -> Supervised classifier\n",
    "# Works with a wide MIMIC-style table that includes DOSE_VAL_RX and abx one-hot flags.\n",
    "# Requires: pandas, numpy, scikit-learn, joblib\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_recall_fscore_support,\n",
    "    classification_report, confusion_matrix, silhouette_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "DATA_PATH = \"output_with_is_dead.csv\"   # <- change if needed\n",
    "RANDOM_STATE = 1337\n",
    "DEFAULT_K = 4\n",
    "TRY_PICK_K = True          # set False to force DEFAULT_K\n",
    "K_CANDIDATES = [3, 4, 5, 6]\n",
    "SIL_SAMPLE_N = 4000        # sample size for silhouette (to keep it fast)\n",
    "MARGIN_PCT = 0.00          # 0.00 = label everyone; e.g., 0.05 ignores ±5% zone around median\n",
    "\n",
    "ID_COLS_CANON = [\"SUBJECT_ID\", \"HADM_ID\", \"ICUSTAY_ID\"]\n",
    "OUTCOME_COLS = [\"is_dead\"]   # always exclude from features\n",
    "CANDIDATE_DOSE_COLS = [\n",
    "    \"dose_val_rx\", \"dose\", \"dosage\", \"dose_val\", \"abx_dose\"\n",
    "]\n",
    "OPTIONAL_CATEGORICAL = [\"Antibiotic_category\"]  # one-hot encode if present\n",
    "\n",
    "MODEL_PATH = \"antibiotic_direction_pipeline.joblib\"\n",
    "SAMPLE_PRED_PATH = \"sample_predictions_first200.csv\"\n",
    "\n",
    "# -------------------------\n",
    "# UTILITIES\n",
    "# -------------------------\n",
    "def find_dose_col(df: pd.DataFrame) -> str:\n",
    "    lower = {c.lower(): c for c in df.columns}\n",
    "    for key in CANDIDATE_DOSE_COLS:\n",
    "        if key in lower:\n",
    "            return lower[key]\n",
    "    raise ValueError(\n",
    "        f\"Could not find dose column; looked for any of {CANDIDATE_DOSE_COLS} \"\n",
    "        f\"in columns={list(df.columns)[:10]}...\"\n",
    "    )\n",
    "\n",
    "def build_preprocessor(df: pd.DataFrame, dose_col: str):\n",
    "    # IDs & outcomes to drop from features\n",
    "    id_cols = [c for c in df.columns if c.upper() in set(ID_COLS_CANON)]\n",
    "    drop_cols = set(id_cols + OUTCOME_COLS + [dose_col])\n",
    "\n",
    "    # numeric vs categorical features\n",
    "    cat_cols = [c for c in OPTIONAL_CATEGORICAL if c in df.columns]\n",
    "    num_cols = [c for c in df.select_dtypes(include=[np.number]).columns\n",
    "                if c not in drop_cols and c not in cat_cols]\n",
    "\n",
    "    # numeric pipeline\n",
    "    numeric_pipeline = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    ])\n",
    "\n",
    "    # categorical pipeline (rare in this table, but include if 'Antibiotic_category' exists)\n",
    "    categorical_pipeline = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_pipeline, num_cols),\n",
    "            (\"cat\", categorical_pipeline, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    feature_cols = num_cols + cat_cols\n",
    "    return preprocess, feature_cols, id_cols\n",
    "\n",
    "def choose_k_by_silhouette(X_mat, k_values, sample_n=SIL_SAMPLE_N, random_state=RANDOM_STATE):\n",
    "    # Sample to speed up silhouette computation\n",
    "    if X_mat.shape[0] > sample_n:\n",
    "        rng = np.random.default_rng(random_state)\n",
    "        idx = rng.choice(X_mat.shape[0], size=sample_n, replace=False)\n",
    "        X_eval = X_mat[idx]\n",
    "    else:\n",
    "        X_eval = X_mat\n",
    "\n",
    "    best_k, best_sil = None, -1.0\n",
    "    for k in k_values:\n",
    "        km = KMeans(n_clusters=k, n_init=10, random_state=random_state)\n",
    "        labels = km.fit_predict(X_eval)\n",
    "        # Silhouette needs at least 2 clusters with >1 sample\n",
    "        if len(set(labels)) < 2:\n",
    "            continue\n",
    "        score = silhouette_score(X_eval, labels, metric=\"euclidean\")\n",
    "        if score > best_sil:\n",
    "            best_sil, best_k = score, k\n",
    "    return best_k if best_k is not None else DEFAULT_K, best_sil\n",
    "\n",
    "def make_labels_by_cluster_median(df: pd.DataFrame, dose_col: str, clusters: np.ndarray, margin_pct: float):\n",
    "    # median per cluster\n",
    "    tmp = df[[dose_col]].copy()\n",
    "    tmp[\"_cluster\"] = clusters\n",
    "    med = tmp.groupby(\"_cluster\")[dose_col].median()\n",
    "\n",
    "    # Broadcast median, compute direction with optional margin\n",
    "    df = df.copy()\n",
    "    df[\"_cluster\"] = clusters\n",
    "    df[\"_cluster_median_dose\"] = df[\"_cluster\"].map(med)\n",
    "    # Label rule: 1 = increase if dose < median * (1 - margin)\n",
    "    #             0 = decrease otherwise\n",
    "    median_adj = df[\"_cluster_median_dose\"] * (1.0 - margin_pct)\n",
    "    y = (df[dose_col] < median_adj).astype(\"Int64\")  # keep NA where dose is NA\n",
    "    # Optionally drop borderline rows when margin>0? We already used a < (1 - margin) threshold\n",
    "    return y, med.to_dict()\n",
    "\n",
    "# -------------------------\n",
    "# MAIN FLOW\n",
    "# -------------------------\n",
    "def main():\n",
    "    # 1) Load\n",
    "    df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "    dose_col = find_dose_col(df)\n",
    "    df[dose_col] = pd.to_numeric(df[dose_col], errors=\"coerce\")\n",
    "\n",
    "    # 2) Preprocess builder\n",
    "    preprocess, feature_cols, id_cols = build_preprocessor(df, dose_col)\n",
    "\n",
    "    # 3) Fit transform features for KMeans\n",
    "    X = df[feature_cols].copy()\n",
    "    X_mat = preprocess.fit_transform(X)\n",
    "\n",
    "    # 4) Pick k (optional) & fit KMeans\n",
    "    if TRY_PICK_K:\n",
    "        k, sil = choose_k_by_silhouette(X_mat, K_CANDIDATES)\n",
    "        print(f\"[k-pick] best k={k} (silhouette on sample ≈ {sil:.3f})\")\n",
    "    else:\n",
    "        k = DEFAULT_K\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k, n_init=20, random_state=RANDOM_STATE)\n",
    "    clusters = kmeans.fit_predict(X_mat)\n",
    "\n",
    "    # 5) Create labels (requires dose)\n",
    "    y_full, cluster_medians = make_labels_by_cluster_median(df, dose_col, clusters, margin_pct=MARGIN_PCT)\n",
    "\n",
    "    # Keep rows that have a valid label (dose not null; label produced)\n",
    "    mask_train = y_full.notna()\n",
    "    X_tr_full = df.loc[mask_train, feature_cols].copy()\n",
    "    y_tr_full = y_full.loc[mask_train].astype(int)\n",
    "\n",
    "    # Refit preprocess on training subset (avoid minor leakage concerns)\n",
    "    preprocess_tr = preprocess.fit(X_tr_full)\n",
    "    X_tr_mat = preprocess_tr.transform(X_tr_full)\n",
    "\n",
    "    # 6) Supervised train/val split\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X_tr_mat, y_tr_full, test_size=0.2, random_state=RANDOM_STATE, stratify=y_tr_full\n",
    "    )\n",
    "\n",
    "    # 7) Train classifier (baseline)\n",
    "    clf = LogisticRegression(\n",
    "        max_iter=1000, class_weight=\"balanced\", solver=\"lbfgs\", random_state=RANDOM_STATE\n",
    "    )\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    y_pred = clf.predict(X_te)\n",
    "\n",
    "    acc = accuracy_score(y_te, y_pred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_te, y_pred, average=\"binary\")\n",
    "    print(\"\\n=== Supervised performance on pseudo-labels (held-out) ===\")\n",
    "    print(f\"Accuracy:  {acc:.3f}\")\n",
    "    print(f\"Precision: {p:.3f}\")\n",
    "    print(f\"Recall:    {r:.3f}\")\n",
    "    print(f\"F1:        {f1:.3f}\")\n",
    "    print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_te, y_pred))\n",
    "    print(\"\\nClassification report:\\n\", classification_report(y_te, y_pred, digits=3))\n",
    "\n",
    "    # 8) Save artifacts\n",
    "    bundle = {\n",
    "        \"preprocess\": preprocess_tr,\n",
    "        \"kmeans\": kmeans,\n",
    "        \"cluster_medians\": cluster_medians,\n",
    "        \"clf\": clf,\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"dose_col\": dose_col,\n",
    "        \"id_cols\": id_cols,\n",
    "        \"k\": k,\n",
    "        \"margin_pct\": MARGIN_PCT,\n",
    "    }\n",
    "    joblib.dump(bundle, MODEL_PATH)\n",
    "    print(f\"\\nSaved model bundle to: {MODEL_PATH}\")\n",
    "\n",
    "    # 9) Demo predictions for first 200 valid rows (maps 0/1 -> decrease/increase)\n",
    "    #    (We use all rows with non-null features; labels are not required for prediction.)\n",
    "    X_all_mat = preprocess_tr.transform(df[feature_cols])\n",
    "    yhat_all = clf.predict(X_all_mat)\n",
    "    dir_map = {0: \"decrease\", 1: \"increase\"}\n",
    "\n",
    "    # ID columns if present, else empty\n",
    "    if all(c in df.columns for c in ID_COLS_CANON):\n",
    "        out = df[ID_COLS_CANON].copy()\n",
    "    else:\n",
    "        out = pd.DataFrame(index=df.index)\n",
    "\n",
    "    out[\"pred_direction\"] = pd.Series(yhat_all, index=df.index).map(dir_map)\n",
    "    out.head(200).to_csv(SAMPLE_PRED_PATH, index=False)\n",
    "    print(f\"Wrote sample predictions to: {SAMPLE_PRED_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9400844b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ed83116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Reading: output_with_is_dead.csv\n",
      "[Info] Shape: (16431, 135)\n",
      "[Info] Using dose column: DOSE_VAL_RX\n",
      "[Info] #numeric feature columns: 121\n",
      "[Info] Clustering with MiniBatchKMeans(k=4) ...\n",
      "[OK] Wrote pseudolabel dataset: C:\\Users\\VishalMaurya\\Downloads\\BTP\\model\\3rd Attempt\\pseudolabel_dataset.csv\n",
      "[OK] Wrote metadata JSON:       C:\\Users\\VishalMaurya\\Downloads\\BTP\\model\\3rd Attempt\\pseudolabel_meta.json\n",
      "[Info] Rows with pseudolabels:  16419 / 16431\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ICUSTAY_ID</th>\n",
       "      <th>DOSE_VAL_RX</th>\n",
       "      <th>_cluster</th>\n",
       "      <th>_cluster_median_dose</th>\n",
       "      <th>abx__*NF* Ertapenem Sodium</th>\n",
       "      <th>abx__AMOXicillin Oral Susp.</th>\n",
       "      <th>abx__AMP</th>\n",
       "      <th>abx__Amikacin</th>\n",
       "      <th>...</th>\n",
       "      <th>abx__Vancomycin Intrathecal</th>\n",
       "      <th>abx__Vancomycin Oral Liquid</th>\n",
       "      <th>abx__ce</th>\n",
       "      <th>abx__demeclocycline</th>\n",
       "      <th>abx__fidaxomicin</th>\n",
       "      <th>abx__fosfomycin tromethamine</th>\n",
       "      <th>abx__moxifloxacin</th>\n",
       "      <th>abx__ofloxacin</th>\n",
       "      <th>_pseudo_label</th>\n",
       "      <th>_pseudo_label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>176176</td>\n",
       "      <td>296681</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>176176</td>\n",
       "      <td>296681</td>\n",
       "      <td>500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>increase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>176176</td>\n",
       "      <td>296681</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>190797</td>\n",
       "      <td>261857</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>decrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>190797</td>\n",
       "      <td>261857</td>\n",
       "      <td>500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>decrease</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID  HADM_ID  ICUSTAY_ID  DOSE_VAL_RX  _cluster  \\\n",
       "0          33   176176      296681       1000.0         1   \n",
       "1          33   176176      296681        500.0         1   \n",
       "2          33   176176      296681       1000.0         1   \n",
       "3          52   190797      261857       2000.0         1   \n",
       "4          52   190797      261857        500.0         2   \n",
       "\n",
       "   _cluster_median_dose  abx__*NF* Ertapenem Sodium  \\\n",
       "0                1000.0                           0   \n",
       "1                1000.0                           0   \n",
       "2                1000.0                           0   \n",
       "3                1000.0                           0   \n",
       "4                 500.0                           0   \n",
       "\n",
       "   abx__AMOXicillin Oral Susp.  abx__AMP  abx__Amikacin  ...  \\\n",
       "0                            0         0              0  ...   \n",
       "1                            0         0              0  ...   \n",
       "2                            0         0              0  ...   \n",
       "3                            0         0              0  ...   \n",
       "4                            0         0              0  ...   \n",
       "\n",
       "   abx__Vancomycin Intrathecal  abx__Vancomycin Oral Liquid  abx__ce  \\\n",
       "0                            0                            0        0   \n",
       "1                            0                            0        0   \n",
       "2                            0                            0        0   \n",
       "3                            0                            0        0   \n",
       "4                            0                            0        0   \n",
       "\n",
       "   abx__demeclocycline  abx__fidaxomicin  abx__fosfomycin tromethamine  \\\n",
       "0                    0                 0                             0   \n",
       "1                    0                 0                             0   \n",
       "2                    0                 0                             0   \n",
       "3                    0                 0                             0   \n",
       "4                    0                 0                             0   \n",
       "\n",
       "   abx__moxifloxacin  abx__ofloxacin  _pseudo_label  _pseudo_label_str  \n",
       "0                  0               0              0           decrease  \n",
       "1                  0               0              1           increase  \n",
       "2                  0               0              0           decrease  \n",
       "3                  0               0              0           decrease  \n",
       "4                  0               0              0           decrease  \n",
       "\n",
       "[5 rows x 129 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from make_pseudolabels import make_pseudolabels\n",
    "\n",
    "# Use your actual CSV path here:\n",
    "RAW = r\"output_with_is_dead.csv\"\n",
    "\n",
    "pseudo_df, meta = make_pseudolabels(\n",
    "    input_path=RAW,\n",
    "    # If you omit these, files are written next to RAW:\n",
    "    output_csv=r\"pseudolabel_dataset.csv\",\n",
    "    meta_json=r\"pseudolabel_meta.json\",\n",
    "    # Optional: you can pass comma strings too, e.g. \"SUBJECT_ID,HADM_ID,ICUSTAY_ID\"\n",
    "    id_cols=[\"SUBJECT_ID\",\"HADM_ID\",\"ICUSTAY_ID\"],\n",
    "    drop_cols=[\"is_dead\"],\n",
    "    k=4,\n",
    "    margin_pct=0.00,\n",
    "    random_state=1337,\n",
    ")\n",
    "\n",
    "pseudo_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa3f6b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Validation vs pseudolabels (held-out) ===\n",
      "Accuracy: 0.97\n",
      "F1: 0.956\n",
      "Confusion:\n",
      " [[2104   40]\n",
      " [  59 1081]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.973     0.981     0.977      2144\n",
      "           1      0.964     0.948     0.956      1140\n",
      "\n",
      "    accuracy                          0.970      3284\n",
      "   macro avg      0.969     0.965     0.967      3284\n",
      "weighted avg      0.970     0.970     0.970      3284\n",
      "\n",
      "[OK] Wrote predictions to: C:\\Users\\VishalMaurya\\Downloads\\BTP\\model\\3rd Attempt\\final_labels_supervised.csv\n",
      "[OK] Saved model bundle: C:\\Users\\VishalMaurya\\Downloads\\BTP\\model\\3rd Attempt\\antibiotic_direction_pipeline.joblib\n"
     ]
    }
   ],
   "source": [
    "# --- Supervised learning on pseudolabels -> write actual predicted labels to CSV ---\n",
    "# Works in Windows/Jupyter. No CLI needed. Just set the paths and run.\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import joblib\n",
    "\n",
    "# ====== EDIT THESE PATHS ======\n",
    "PSEUDO_CSV  = r\"pseudolabel_dataset.csv\"          # produced by your make_pseudolabels step\n",
    "OUTPUT_CSV  = r\"final_labels_supervised.csv\"      # where to save predictions\n",
    "MODEL_PATH  = r\"antibiotic_direction_pipeline.joblib\"  # optional: save model bundle\n",
    "META_JSON   = r\"pseudolabel_meta.json\"            # optional: if present, used to lock feature set\n",
    "PREDICT_ON  = None  # set to r\"C:\\path\\to\\original_or_new_batch.csv\" to predict elsewhere; or keep None\n",
    "\n",
    "# ====== LOADING ======\n",
    "pseudo_df = pd.read_csv(PSEUDO_CSV, low_memory=False)\n",
    "if \"_pseudo_label\" not in pseudo_df.columns:\n",
    "    raise ValueError(\"pseudolabel CSV must contain a '_pseudo_label' column.\")\n",
    "\n",
    "# Optional: load feature list from meta JSON (if you created it earlier)\n",
    "feat_cols_from_meta = None\n",
    "try:\n",
    "    meta_p = Path(META_JSON)\n",
    "    if meta_p.exists():\n",
    "        meta = json.loads(meta_p.read_text(encoding=\"utf-8\"))\n",
    "        feat_cols_from_meta = meta.get(\"feature_cols\", None)\n",
    "        # optional ids to include in output (if present)\n",
    "        id_cols_from_meta = meta.get(\"id_cols\", [])\n",
    "    else:\n",
    "        id_cols_from_meta = [\"SUBJECT_ID\", \"HADM_ID\", \"ICUSTAY_ID\"]\n",
    "except Exception:\n",
    "    id_cols_from_meta = [\"SUBJECT_ID\", \"HADM_ID\", \"ICUSTAY_ID\"]\n",
    "\n",
    "# ====== FEATURE SELECTION (numeric-only, robust fallback if no meta) ======\n",
    "EXCLUDE_BASE = {\n",
    "    \"_pseudo_label\", \"_pseudo_label_str\",\n",
    "    \"_cluster\", \"_cluster_median_dose\",\n",
    "    \"dose_val_rx\", \"dose\", \"dosage\", \"dose_val\", \"abx_dose\"\n",
    "}\n",
    "EXCLUDE_BASE |= set([c for c in id_cols_from_meta if c in pseudo_df.columns])\n",
    "\n",
    "if feat_cols_from_meta:\n",
    "    # use meta-driven features (safer when predicting on new data)\n",
    "    missing = [c for c in feat_cols_from_meta if c not in pseudo_df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Pseudo CSV missing expected feature columns from meta: {missing[:15]} ...\")\n",
    "    feature_cols = feat_cols_from_meta\n",
    "else:\n",
    "    # infer: all numeric columns except excluded\n",
    "    numeric_cols = list(pseudo_df.select_dtypes(include=[np.number]).columns)\n",
    "    feature_cols = [c for c in numeric_cols if c not in EXCLUDE_BASE]\n",
    "\n",
    "if not feature_cols:\n",
    "    raise ValueError(\"No feature columns found to train on. Check your pseudolabel CSV.\")\n",
    "\n",
    "# ====== PREPROCESS + TRAIN ======\n",
    "X_all = pseudo_df[feature_cols].copy()\n",
    "y_all = pseudo_df[\"_pseudo_label\"].astype(int).values\n",
    "\n",
    "preprocess = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "X_all_proc = preprocess.fit_transform(X_all)\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X_all_proc, y_all, test_size=0.20, random_state=42, stratify=y_all\n",
    ")\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    max_iter=1000, class_weight=\"balanced\", solver=\"lbfgs\", random_state=42\n",
    ")\n",
    "clf.fit(X_tr, y_tr)\n",
    "\n",
    "# quick validation (vs pseudolabels)\n",
    "y_pred = clf.predict(X_te)\n",
    "print(\"=== Validation vs pseudolabels (held-out) ===\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y_te, y_pred), 3))\n",
    "print(\"F1:\", round(f1_score(y_te, y_pred), 3))\n",
    "print(\"Confusion:\\n\", confusion_matrix(y_te, y_pred))\n",
    "print(classification_report(y_te, y_pred, digits=3))\n",
    "\n",
    "# ====== PREDICT TARGET TABLE ======\n",
    "if PREDICT_ON:\n",
    "    target_df = pd.read_csv(PREDICT_ON, low_memory=False)\n",
    "    # ensure feature set exists\n",
    "    miss2 = [c for c in feature_cols if c not in target_df.columns]\n",
    "    if miss2:\n",
    "        raise ValueError(f\"'predict_on' CSV missing feature columns: {miss2[:15]} ...\")\n",
    "    X_pred_df = target_df[feature_cols].copy()\n",
    "    id_cols_present = [c for c in id_cols_from_meta if c in target_df.columns]\n",
    "else:\n",
    "    target_df = pseudo_df\n",
    "    X_pred_df = target_df[feature_cols].copy()\n",
    "    id_cols_present = [c for c in id_cols_from_meta if c in target_df.columns]\n",
    "\n",
    "X_pred = preprocess.transform(X_pred_df)\n",
    "yhat = clf.predict(X_pred)\n",
    "proba_inc = clf.predict_proba(X_pred)[:, 1]\n",
    "\n",
    "# ====== WRITE OUTPUT ======\n",
    "out = target_df[id_cols_present].copy() if id_cols_present else pd.DataFrame(index=target_df.index)\n",
    "out[\"final_label\"] = np.where(yhat == 1, \"increase\", \"decrease\")\n",
    "out[\"final_label_id\"] = yhat\n",
    "out[\"final_label_prob_increase\"] = proba_inc\n",
    "\n",
    "out_path = Path(OUTPUT_CSV)\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "out.to_csv(out_path, index=False)\n",
    "print(f\"[OK] Wrote predictions to: {out_path.resolve()}\")\n",
    "\n",
    "# ====== SAVE MODEL BUNDLE (optional) ======\n",
    "joblib.dump({\n",
    "    \"preprocess\": preprocess,\n",
    "    \"clf\": clf,\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"id_cols\": id_cols_from_meta\n",
    "}, MODEL_PATH)\n",
    "print(f\"[OK] Saved model bundle: {Path(MODEL_PATH).resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
